{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s22398/GS2DIT/blob/main/Class%203/gpt_2_shakespeare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: \n",
        "\n",
        "*   https://pypi.org/project/gpt-2-simple/#description\n",
        "*   https://medium.com/@stasinopoulos.dimitrios/a-beginners-guide-to-training-and-generating-text-using-gpt2-c2f2e1fbd10a\n",
        "*   https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce#scrollTo=VHdTL8NDbAh3\n",
        "*  https://github.com/ak9250/gpt-2-colab\n",
        "*  https://www.aiweirdness.com/d-and-d-character-bios-now-making-19-03-15/\n",
        "*  https://minimaxir.com/2019/09/howto-gpt2/\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rgNM-NcAZ9aT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/zawemi/GS2DIT/blob/main/Class%203/gpt_2_shakespeare.ipynb#scrollTo=4tIUvFbLMUuE)"
      ],
      "metadata": {
        "id": "4tIUvFbLMUuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Let's teach AI writing like a Shakespeare ðŸŽ“"
      ],
      "metadata": {
        "id": "MofLJqBHAWXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Installing the model"
      ],
      "metadata": {
        "id": "W7wiPFGQQn9o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQACJ8lyUIR0",
        "outputId": "c43041a5-8140-4bc1-a8a1-e6e2b33f2ec4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gpt-2-simple\n",
            "  Downloading gpt_2_simple-0.8.1.tar.gz (26 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow>=2.5.1 in /usr/local/lib/python3.9/dist-packages (from gpt-2-simple) (2.11.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from gpt-2-simple) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from gpt-2-simple) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from gpt-2-simple) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from gpt-2-simple) (1.22.4)\n",
            "Collecting toposort\n",
            "  Downloading toposort-1.10-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (23.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.3.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (15.0.6.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.8.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.11.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.51.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (63.4.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.11.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.6.3)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.11.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (4.5.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.2.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.15.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.31.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.19.6)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->gpt-2-simple) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->gpt-2-simple) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->gpt-2-simple) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->gpt-2-simple) (3.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow>=2.5.1->gpt-2-simple) (0.40.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (2.16.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (2.2.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (6.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (3.2.2)\n",
            "Building wheels for collected packages: gpt-2-simple\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpt-2-simple: filename=gpt_2_simple-0.8.1-py3-none-any.whl size=24576 sha256=038a2a999e1e107a2a596b510984d6db9b68b14530d72499425658557d73389f\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/28/f0/2f12e470be10d6804b193e4193d274c88995010fae512a67cf\n",
            "Successfully built gpt-2-simple\n",
            "Installing collected packages: toposort, gpt-2-simple\n",
            "Successfully installed gpt-2-simple-0.8.1 toposort-1.10\n"
          ]
        }
      ],
      "source": [
        "#install the library we'll use today\n",
        "!pip install gpt-2-simple"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating text with basic model"
      ],
      "metadata": {
        "id": "ADzeFwzaQ8cT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing and loading necessary components"
      ],
      "metadata": {
        "id": "d6Ah3D1CRK6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import what we need\n",
        "import gpt_2_simple as gpt2 #for gpt-2 (our AI model)\n",
        "import os #lets us doing things with files and folders\n",
        "import requests #this one helps to dowload from the internet"
      ],
      "metadata": {
        "id": "mLg4pTPDaJJV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#and let's download our AI model\n",
        "gpt2.download_gpt2()   # model is saved into current directory under /models/124M/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIXHjaxvaWsV",
        "outputId": "732685f8-ae98-4d46-c16e-c2f2956c8974"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 360Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:01, 562kit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 874Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:58, 8.47Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 302Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:01, 844kit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:01, 819kit/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#strating the session so we can play with the gpt-2 model\n",
        "sess = gpt2.start_tf_sess()"
      ],
      "metadata": {
        "id": "6CCkn75KbBpg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we load the model from file to use it\n",
        "gpt2.load_gpt2(sess, run_name='124M', checkpoint_dir='models')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsBvHQsxZsyP",
        "outputId": "02d3f7db-66d4-483b-b0fc-84bbf42072a1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text generation"
      ],
      "metadata": {
        "id": "mDSFDj78RQJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this is how we would start model statement\n",
        "prefix = \"I don't know\""
      ],
      "metadata": {
        "id": "-P5_fxZOgGlk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the model is generating text\n",
        "gpt2.generate(sess, run_name='124M', checkpoint_dir='models', prefix=prefix, length=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSYqTat0gNDo",
        "outputId": "37826766-3f80-4094-f861-3c2a638d2c5d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I don't know if I've ever done and said that before.\"\n",
            "\n",
            "But Coyle, a former New York City police chief who last year became the first black police officer at a major city police department, said he's a liberal Democrat.\n",
            "\n",
            "\"I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating text with improved (finetuned) model"
      ],
      "metadata": {
        "id": "ML5helfmRjT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT**\n",
        "</br>Restart the runtime (Runtime -> Restart runtime)"
      ],
      "metadata": {
        "id": "8cEaZKtRPx0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing and loading necessary components"
      ],
      "metadata": {
        "id": "NIPDKskeR7i3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import what we need\n",
        "import gpt_2_simple as gpt2 #for gpt-2 (our AI model)\n",
        "import os #lets us doing things with files and folders\n",
        "import requests #this one helps to dowload from the internet"
      ],
      "metadata": {
        "id": "eHys5-bWPnhJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get nietzsche texts\n",
        "!wget \"https://s3.amazonaws.com/text-datasets/nietzsche.txt\""
      ],
      "metadata": {
        "id": "dRTQyR7IqaOl",
        "outputId": "a461b4c5-225d-4d9b-c6a5-56ce40c108b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-22 12:42:56--  https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.152.182, 54.231.129.224, 52.217.39.78, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.152.182|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 600901 (587K) [text/plain]\n",
            "Saving to: â€˜nietzsche.txtâ€™\n",
            "\n",
            "nietzsche.txt       100%[===================>] 586.82K   542KB/s    in 1.1s    \n",
            "\n",
            "2023-03-22 12:42:58 (542 KB/s) - â€˜nietzsche.txtâ€™ saved [600901/600901]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#game of thrones from https://www.kaggle.com/datasets/khulasasndh/game-of-thrones-books?select=001ssb.txt\n",
        "!gdown \"1CrL1wde_NGO68i5Prd_UNA_oW0cGQsxg&confirm=t\"\n",
        "!mv /content/001ssb.txt /content/got1.txt"
      ],
      "metadata": {
        "id": "pzDNTjJzuKDW",
        "outputId": "f3ef6d4d-e532-4cdd-ecf8-593a227d6d7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CrL1wde_NGO68i5Prd_UNA_oW0cGQsxg&confirm=t\n",
            "To: /content/001ssb.txt\n",
            "\r  0% 0.00/1.63M [00:00<?, ?B/s]\r100% 1.63M/1.63M [00:00<00:00, 167MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#let's dowload a file with all Shakespeare plays\n",
        "!wget \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "!mv /content/input.txt /content/shakespeare.txt"
      ],
      "metadata": {
        "id": "9pwWGn5eqBJn",
        "outputId": "45866a5b-9f12-4666-c103-4298594264ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-22 12:43:00--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: â€˜input.txtâ€™\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.008s  \n",
            "\n",
            "2023-03-22 12:43:00 (139 MB/s) - â€˜input.txtâ€™ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#strating the session so we can play with the gpt-2 model\n",
        "sess = gpt2.start_tf_sess()"
      ],
      "metadata": {
        "id": "A0T2s8RxPnVr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Teaching our model"
      ],
      "metadata": {
        "id": "bvllQvFxR9z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#finetuning with shakespeare.txt (which, to be honest, means that we are teaching the model how to write like a shakespeare)\n",
        "#it takes a lot of time (~15min)...\n",
        "gpt2.finetune(sess, 'nietzsche.txt', steps=500)   # steps is max number of training steps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RJetxF6UOfY",
        "outputId": "3808d677-970f-46d1-92b1-43aa37a14184"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset has 143770 tokens\n",
            "Training...\n",
            "[1 | 7.10] loss=4.01 avg=4.01\n",
            "[2 | 9.22] loss=4.02 avg=4.02\n",
            "[3 | 11.34] loss=4.01 avg=4.01\n",
            "[4 | 13.48] loss=3.83 avg=3.97\n",
            "[5 | 15.62] loss=3.74 avg=3.92\n",
            "[6 | 17.77] loss=3.87 avg=3.91\n",
            "[7 | 19.92] loss=3.82 avg=3.90\n",
            "[8 | 22.08] loss=3.74 avg=3.88\n",
            "[9 | 24.24] loss=3.77 avg=3.87\n",
            "[10 | 26.41] loss=3.51 avg=3.83\n",
            "[11 | 28.59] loss=3.73 avg=3.82\n",
            "[12 | 30.78] loss=3.73 avg=3.81\n",
            "[13 | 32.96] loss=3.76 avg=3.81\n",
            "[14 | 35.15] loss=3.65 avg=3.79\n",
            "[15 | 37.35] loss=3.64 avg=3.78\n",
            "[16 | 39.56] loss=3.52 avg=3.77\n",
            "[17 | 41.77] loss=3.48 avg=3.75\n",
            "[18 | 43.99] loss=3.60 avg=3.74\n",
            "[19 | 46.20] loss=3.45 avg=3.72\n",
            "[20 | 48.43] loss=3.57 avg=3.71\n",
            "[21 | 50.66] loss=3.65 avg=3.71\n",
            "[22 | 52.91] loss=3.68 avg=3.71\n",
            "[23 | 55.15] loss=3.53 avg=3.70\n",
            "[24 | 57.42] loss=3.72 avg=3.70\n",
            "[25 | 59.68] loss=3.46 avg=3.69\n",
            "[26 | 61.94] loss=3.64 avg=3.69\n",
            "[27 | 64.23] loss=3.52 avg=3.68\n",
            "[28 | 66.51] loss=3.49 avg=3.67\n",
            "[29 | 68.79] loss=3.54 avg=3.67\n",
            "[30 | 71.07] loss=3.53 avg=3.66\n",
            "[31 | 73.34] loss=3.45 avg=3.66\n",
            "[32 | 75.60] loss=3.56 avg=3.65\n",
            "[33 | 77.85] loss=3.68 avg=3.65\n",
            "[34 | 80.10] loss=3.51 avg=3.65\n",
            "[35 | 82.35] loss=3.39 avg=3.64\n",
            "[36 | 84.59] loss=3.38 avg=3.63\n",
            "[37 | 86.81] loss=3.52 avg=3.63\n",
            "[38 | 89.04] loss=3.21 avg=3.61\n",
            "[39 | 91.26] loss=3.42 avg=3.61\n",
            "[40 | 93.48] loss=3.32 avg=3.60\n",
            "[41 | 95.70] loss=3.51 avg=3.60\n",
            "[42 | 97.91] loss=3.16 avg=3.58\n",
            "[43 | 100.12] loss=3.63 avg=3.59\n",
            "[44 | 102.33] loss=3.35 avg=3.58\n",
            "[45 | 104.54] loss=3.60 avg=3.58\n",
            "[46 | 106.75] loss=3.63 avg=3.58\n",
            "[47 | 108.95] loss=3.40 avg=3.58\n",
            "[48 | 111.15] loss=3.25 avg=3.57\n",
            "[49 | 113.37] loss=3.28 avg=3.56\n",
            "[50 | 115.58] loss=3.26 avg=3.55\n",
            "[51 | 117.79] loss=3.30 avg=3.55\n",
            "[52 | 120.00] loss=3.36 avg=3.54\n",
            "[53 | 122.21] loss=3.33 avg=3.54\n",
            "[54 | 124.43] loss=3.27 avg=3.53\n",
            "[55 | 126.65] loss=3.35 avg=3.53\n",
            "[56 | 128.86] loss=3.40 avg=3.52\n",
            "[57 | 131.09] loss=3.33 avg=3.52\n",
            "[58 | 133.31] loss=3.25 avg=3.51\n",
            "[59 | 135.54] loss=3.30 avg=3.51\n",
            "[60 | 137.78] loss=3.36 avg=3.50\n",
            "[61 | 140.01] loss=3.37 avg=3.50\n",
            "[62 | 142.24] loss=3.48 avg=3.50\n",
            "[63 | 144.48] loss=3.31 avg=3.50\n",
            "[64 | 146.72] loss=3.14 avg=3.49\n",
            "[65 | 149.01] loss=3.29 avg=3.49\n",
            "[66 | 151.27] loss=3.18 avg=3.48\n",
            "[67 | 153.50] loss=3.16 avg=3.47\n",
            "[68 | 155.73] loss=3.31 avg=3.47\n",
            "[69 | 157.97] loss=3.17 avg=3.46\n",
            "[70 | 160.20] loss=3.17 avg=3.46\n",
            "[71 | 162.43] loss=2.88 avg=3.45\n",
            "[72 | 164.66] loss=3.18 avg=3.44\n",
            "[73 | 166.90] loss=3.04 avg=3.43\n",
            "[74 | 169.13] loss=3.18 avg=3.43\n",
            "[75 | 171.36] loss=3.30 avg=3.43\n",
            "[76 | 173.60] loss=3.02 avg=3.42\n",
            "[77 | 175.82] loss=3.29 avg=3.42\n",
            "[78 | 178.04] loss=3.24 avg=3.41\n",
            "[79 | 180.27] loss=3.25 avg=3.41\n",
            "[80 | 182.49] loss=3.16 avg=3.41\n",
            "[81 | 184.72] loss=3.30 avg=3.40\n",
            "[82 | 186.94] loss=2.94 avg=3.40\n",
            "[83 | 189.17] loss=3.40 avg=3.40\n",
            "[84 | 191.39] loss=3.14 avg=3.39\n",
            "[85 | 193.61] loss=3.03 avg=3.38\n",
            "[86 | 195.83] loss=3.06 avg=3.38\n",
            "[87 | 198.05] loss=3.12 avg=3.37\n",
            "[88 | 200.27] loss=3.23 avg=3.37\n",
            "[89 | 202.49] loss=2.97 avg=3.37\n",
            "[90 | 204.71] loss=3.07 avg=3.36\n",
            "[91 | 206.94] loss=3.03 avg=3.35\n",
            "[92 | 209.16] loss=3.05 avg=3.35\n",
            "[93 | 211.39] loss=3.18 avg=3.35\n",
            "[94 | 213.62] loss=2.91 avg=3.34\n",
            "[95 | 215.85] loss=2.93 avg=3.33\n",
            "[96 | 218.07] loss=2.86 avg=3.33\n",
            "[97 | 220.30] loss=2.74 avg=3.32\n",
            "[98 | 222.53] loss=3.05 avg=3.31\n",
            "[99 | 224.76] loss=2.89 avg=3.31\n",
            "[100 | 226.98] loss=2.71 avg=3.30\n",
            "======== SAMPLE 1 ========\n",
            "ating what is right, what is wrong, what is the greatest delight in being wrong, which is what I am, and yet what I want to be: here and there I find an old morality, a new, new, new, old school of morals, which I have been longing for which I am in the process of retiring to--at last. There were a great deal of books on morals among the learned; but almost all were not very good.\n",
            "\n",
            "230. It seems that the most remarkable thing about the English age--and one of the most interesting and perplexing kinds of modern knowledge--is the degree of intellectual rigidity--the degree of profound and intellectual introspection, of the tendency to persistently yield to skepticism, of a kind also known as intellectual isolation--there is a certain strength of personality, and especially of a tendency which in certain forms of thought is capable of sustaining itself constantly in opposition to all intellectual rigidity in the opposite sense of the old word, it--the English intellectual. This intellectual isolation, which, when it happens, is really a kind of incapacity for thinking, a symptom of a much more extensive and enduring psychological condition, a psychological incapacity for intellectual capacity, a phenomenon in the domain of which the history of the English intellectual period does not disclose, but seems to lie beneath the surface in a more or less arbitrary and isolated form, with the result that, in all the English scientific epochs, the entire intellectual capacity for thought is apparently limited, as is usually believed, to an average of four or five, or to a certain degree of intellectual detachment. All these periods--the discovery and application of new possibilities to existing ones--are accompanied by a period during which the faculties of the mind are, under certain superficial conditions, almost completely closed off, almost entirely silenced and restrained--a period in which the whole intellectual capacity for thought has gradually become, in every respect, the least disposed of, as the domain is becoming more and more open to new possibilities, and consequently the domain is becoming more \"comprehensive\". Between these periods lies a much more difficult period in view of which it is quite probable that the intellectual capacity for thought has yet to grow enormously sufficiently strong to constitute the \"greatest and lastest\" power capable of carrying on a new and more powerful domain within the domain of thought, as the case may be. The present period is this period of enormous intellectual freedom; the whole of it, and probably much of the period after that, may well be called \"the intellectual period,\" for even in the most advanced countries of Europe, in which the average age of thought has never been reached, such a thing exists--but the period in question--has thus far been this \"greatest and lastest\". In fact, the greatest and last great and last great capacity of modern humanity to bear on himself and to persistently and rigorously to the utmost skepticism, to the utmost intellectual rigidity of thought, is generally held as a characteristic of the time, a characteristic in the case of the English intellectual period,--it is perhaps a little improbable that the capacity so highly developed that it might have been thought of as an indication of the domain of human intellectual capacity would become extinct as early as the present century:--what is the point of such a thing? The point is that the capacity for thought has grown enormously fast, fast enough, sufficiently strong enough to become capable of carrying on a new domain of thought,--it might be called \"the intellectual period,\" in relation to which we are now placed: it, too, will certainly be called \"the intellectual period\" as compared with the period in question. Indeed, as has been thought, the capacity for thought is, to one degree or another, absolutely unqualified to enter the domain of thought. One may say thus, \"The capacity for thought has grown enormously, fast enough, sufficiently strong for its turning into definite and definite powers of thought. If the domain of thought were a longer, narrower, less specialized domain with a narrower domain for thought to traverse, it might now be called \"the intellectual period.\" \" \"--What happens? The domain of thought is shut up. The capacity of \"thought\" remains a little too small for its turning into definite and definite powers of thought to traverse; and as it is in the highest degree prepossessed for the carrying out of new, new, new domain of thought, the domain of thought itself has become, for a time, essentially prepossessed, for carrying on a previous and already large, formerly largely prepossessed, domain of thought--and a great deal is still left of the world of thought and experience.--That which takes care of itself, however, does not--does not belong to itself,--but remains as such--is not there thereby for whom one can say, \"A new and new domain of thought is to be conceived; there is, on this account, nothing for it to be--a new, new, new domain of thought\"? There is something in\n",
            "\n",
            "[101 | 241.52] loss=2.88 avg=3.29\n",
            "[102 | 243.75] loss=3.09 avg=3.29\n",
            "[103 | 245.98] loss=2.91 avg=3.28\n",
            "[104 | 248.21] loss=3.31 avg=3.28\n",
            "[105 | 250.44] loss=2.96 avg=3.28\n",
            "[106 | 252.67] loss=2.78 avg=3.27\n",
            "[107 | 254.90] loss=3.01 avg=3.26\n",
            "[108 | 257.14] loss=2.83 avg=3.26\n",
            "[109 | 259.38] loss=2.84 avg=3.25\n",
            "[110 | 261.61] loss=3.21 avg=3.25\n",
            "[111 | 263.84] loss=2.82 avg=3.24\n",
            "[112 | 266.07] loss=2.61 avg=3.23\n",
            "[113 | 268.30] loss=3.05 avg=3.23\n",
            "[114 | 270.54] loss=2.88 avg=3.23\n",
            "[115 | 272.77] loss=2.71 avg=3.22\n",
            "[116 | 275.00] loss=2.93 avg=3.22\n",
            "[117 | 277.23] loss=2.88 avg=3.21\n",
            "[118 | 279.46] loss=2.89 avg=3.21\n",
            "[119 | 281.69] loss=2.63 avg=3.20\n",
            "[120 | 283.92] loss=2.84 avg=3.19\n",
            "[121 | 286.15] loss=2.92 avg=3.19\n",
            "[122 | 288.38] loss=2.46 avg=3.18\n",
            "[123 | 290.62] loss=2.69 avg=3.17\n",
            "[124 | 292.86] loss=2.77 avg=3.17\n",
            "[125 | 295.10] loss=2.50 avg=3.16\n",
            "[126 | 297.33] loss=2.83 avg=3.15\n",
            "[127 | 299.55] loss=2.86 avg=3.15\n",
            "[128 | 301.78] loss=2.80 avg=3.14\n",
            "[129 | 304.01] loss=2.67 avg=3.14\n",
            "[130 | 306.26] loss=2.84 avg=3.13\n",
            "[131 | 308.50] loss=2.91 avg=3.13\n",
            "[132 | 310.73] loss=2.55 avg=3.12\n",
            "[133 | 312.96] loss=2.36 avg=3.11\n",
            "[134 | 315.19] loss=2.38 avg=3.10\n",
            "[135 | 317.42] loss=2.70 avg=3.10\n",
            "[136 | 319.65] loss=2.86 avg=3.09\n",
            "[137 | 321.89] loss=2.82 avg=3.09\n",
            "[138 | 324.12] loss=2.41 avg=3.08\n",
            "[139 | 326.35] loss=2.75 avg=3.08\n",
            "[140 | 328.58] loss=2.77 avg=3.07\n",
            "[141 | 330.82] loss=2.56 avg=3.06\n",
            "[142 | 333.06] loss=2.78 avg=3.06\n",
            "[143 | 335.29] loss=2.85 avg=3.06\n",
            "[144 | 337.52] loss=2.55 avg=3.05\n",
            "[145 | 339.75] loss=2.67 avg=3.05\n",
            "[146 | 341.98] loss=2.80 avg=3.04\n",
            "[147 | 344.21] loss=2.72 avg=3.04\n",
            "[148 | 346.44] loss=2.42 avg=3.03\n",
            "[149 | 348.67] loss=2.60 avg=3.03\n",
            "[150 | 350.90] loss=2.58 avg=3.02\n",
            "[151 | 353.13] loss=2.59 avg=3.01\n",
            "[152 | 355.37] loss=2.58 avg=3.01\n",
            "[153 | 357.60] loss=2.55 avg=3.00\n",
            "[154 | 359.83] loss=2.30 avg=2.99\n",
            "[155 | 362.06] loss=2.80 avg=2.99\n",
            "[156 | 364.29] loss=2.80 avg=2.99\n",
            "[157 | 366.53] loss=2.91 avg=2.99\n",
            "[158 | 368.76] loss=2.30 avg=2.98\n",
            "[159 | 371.00] loss=2.45 avg=2.97\n",
            "[160 | 373.23] loss=2.42 avg=2.97\n",
            "[161 | 375.46] loss=2.53 avg=2.96\n",
            "[162 | 377.69] loss=2.31 avg=2.95\n",
            "[163 | 379.93] loss=2.77 avg=2.95\n",
            "[164 | 382.15] loss=2.49 avg=2.94\n",
            "[165 | 384.38] loss=2.36 avg=2.94\n",
            "[166 | 386.62] loss=2.12 avg=2.93\n",
            "[167 | 388.85] loss=2.28 avg=2.92\n",
            "[168 | 391.08] loss=2.53 avg=2.91\n",
            "[169 | 393.31] loss=2.49 avg=2.91\n",
            "[170 | 395.55] loss=2.43 avg=2.90\n",
            "[171 | 397.78] loss=2.25 avg=2.90\n",
            "[172 | 400.01] loss=2.25 avg=2.89\n",
            "[173 | 402.24] loss=2.44 avg=2.88\n",
            "[174 | 404.48] loss=2.36 avg=2.88\n",
            "[175 | 406.71] loss=2.20 avg=2.87\n",
            "[176 | 408.94] loss=2.20 avg=2.86\n",
            "[177 | 411.17] loss=2.05 avg=2.85\n",
            "[178 | 413.40] loss=1.93 avg=2.84\n",
            "[179 | 415.63] loss=2.11 avg=2.83\n",
            "[180 | 417.86] loss=2.26 avg=2.82\n",
            "[181 | 420.09] loss=2.18 avg=2.82\n",
            "[182 | 422.32] loss=1.92 avg=2.80\n",
            "[183 | 424.55] loss=2.44 avg=2.80\n",
            "[184 | 426.78] loss=2.02 avg=2.79\n",
            "[185 | 429.02] loss=2.07 avg=2.78\n",
            "[186 | 431.25] loss=2.16 avg=2.78\n",
            "[187 | 433.48] loss=2.21 avg=2.77\n",
            "[188 | 435.71] loss=1.66 avg=2.76\n",
            "[189 | 437.94] loss=2.21 avg=2.75\n",
            "[190 | 440.18] loss=2.45 avg=2.75\n",
            "[191 | 442.41] loss=2.37 avg=2.74\n",
            "[192 | 444.64] loss=1.93 avg=2.73\n",
            "[193 | 446.87] loss=2.22 avg=2.73\n",
            "[194 | 449.10] loss=1.98 avg=2.72\n",
            "[195 | 451.34] loss=2.29 avg=2.71\n",
            "[196 | 453.57] loss=1.77 avg=2.70\n",
            "[197 | 455.81] loss=1.96 avg=2.69\n",
            "[198 | 458.04] loss=2.24 avg=2.69\n",
            "[199 | 460.27] loss=2.14 avg=2.68\n",
            "[200 | 462.51] loss=1.81 avg=2.67\n",
            "======== SAMPLE 1 ========\n",
            " its pleasure.\n",
            "\"In the spirit of the\n",
            "free, fearless Jew, who wears the cross in all the pomp and\n",
            "attitude; in the noble heart, where all the sorrow and\n",
            "sin has been concealed, where death is a reality; in the sight,\n",
            "immorality, and soul of the lawless, boundless, and all the\n",
            "soul of the ever-living--whom all hope and loathing must now\n",
            "inspire--how low, for the sake of which such a lofty ideal\n",
            "pals still so proudly! How weak, how un-Homeric! How monstrositous!\n",
            "\n",
            "\n",
            "14\n",
            "\n",
            "From which it has flowed all its life, yet never one of its\n",
            "sustains as one of nobility, that famous golden age of\n",
            "philosophy, when the most holy and joyous feelings and\n",
            "insight diffused themselves without rancour. To-day the\n",
            "taste of pure, pure, geraniumular sentimentality reigns over the\n",
            "strict and free spirit, that relaxed energy, and\n",
            "manliness have taken root anew among its many recessies, in\n",
            "order to retain its essence and remain the drink and\n",
            "treasure of society and its convalescences, as even the philosopher\n",
            "Pascalilides declared.--To retain its essence, to keep hold of\n",
            "it by means of another man, in short, to be free from the\n",
            "original distrust and suspicion of the \"noble\" and its admirers and\n",
            "saint, to remain unalterably true to the spirit of a father,\n",
            "and without fear that it will be all out of tune with its children\n",
            "even if they did obey. To retain its secrecy, to keep\n",
            "it in a corner and wait till its children learnt to speak and write\n",
            "before speaking and writing again--to take care lest its children\n",
            "be mistaken. To remain as a noble idleness and secrecy around\n",
            "each and every one, in case one or other of its children is\n",
            "betrayed in the greatest tragedy, sickness, accident, or\n",
            "heroism, as a cripple, in whose hands and feet its beloved\n",
            "Empiricism and Freethinkerbewegung are in act and piece. To have \"\n",
            "disadvantage\" as your child's privilege, as your father's right and\n",
            "privilege, as your paternal right and \"equality,\" as\n",
            "your father's duty and majesty, as your fundamental distrust, fear,\n",
            "honour, and loathing--that is to be DISBONENTS.--WILL I RECOGNEE YOU\n",
            "FOR A TEXTILE?\n",
            "\n",
            "15. It may be said in justice, that even in our deepest affinities,\n",
            "our deepest prejudices have not, in the longest period of our\n",
            "development, encouraged each other to war and tears; we laugh at\n",
            "stupidities of every character, impiety of every idea, covetousness and\n",
            "insight, which are injurious to the stomach, we who have only recently\n",
            "come to regard all things as bad--that has been our most\n",
            "vivacious long defence. Let us never again allow ourselves to be\n",
            "condemned for having such narrow views, for having a bad conscience, or\n",
            "for being partial to good people and bad people, or for being afraid of bad\n",
            "people, for all of these things. Let us certainly, in fact, remove\n",
            "our deepest grudges; let us look forward to a cheerful season, our\n",
            "first definite goal, our greatest achievement; let us keep our eyes fixed on\n",
            "the wind, our deepest breath, the breath of life which blows with a clean\n",
            "conscience, that blows with a steady and fearless will, that which\n",
            "cannot deceive, and feels no end, no injury. Let us, therefore, strive\n",
            "for knowledge, for the spirit of youth and knowledge which no longer\n",
            "loves itself and its facts; for the holy secret of knowledge, the\n",
            "spirit of patience and sagacity which all good people lack--for, who\n",
            "spoke \"to the spirit of good people\"!\n",
            "\n",
            "\n",
            "16\n",
            "\n",
            "=The Asceticism of Schopenhauer.=--Hitherto it has been generally\n",
            "understated that Schopenhauer is neither the first nor last in the\n",
            "highest estimation of moralists and pessimists in regard to\n",
            "happiness; for the whole conception of man according to his\n",
            "circumstances and circumstances is still essentially infirm and\n",
            "hitherto has been rendered in error and in disrepute by other\n",
            "theories. A consideration which has hitherto been lacking in the lower\n",
            "society about the relation and nature of man to his natural\n",
            "quarters is now prevalent in contemporary moralists; they believe\n",
            "that man is made bad when he does bad things and is therefore therefore\n",
            "impure. This belief in the utility of certain things in man, in himself\n",
            "or in his ego, is regarded as the root cause of the marked\n",
            "\n",
            "[201 | 475.44] loss=1.80 avg=2.66\n",
            "[202 | 477.67] loss=1.87 avg=2.65\n",
            "[203 | 479.90] loss=2.21 avg=2.65\n",
            "[204 | 482.13] loss=2.02 avg=2.64\n",
            "[205 | 484.36] loss=1.57 avg=2.63\n",
            "[206 | 486.59] loss=2.10 avg=2.62\n",
            "[207 | 488.82] loss=1.87 avg=2.61\n",
            "[208 | 491.05] loss=1.79 avg=2.60\n",
            "[209 | 493.27] loss=1.91 avg=2.60\n",
            "[210 | 495.50] loss=1.88 avg=2.59\n",
            "[211 | 497.74] loss=2.16 avg=2.58\n",
            "[212 | 499.97] loss=2.18 avg=2.58\n",
            "[213 | 502.20] loss=1.96 avg=2.57\n",
            "[214 | 504.43] loss=2.13 avg=2.57\n",
            "[215 | 506.65] loss=1.93 avg=2.56\n",
            "[216 | 508.88] loss=2.22 avg=2.55\n",
            "[217 | 511.12] loss=1.45 avg=2.54\n",
            "[218 | 513.35] loss=1.80 avg=2.53\n",
            "[219 | 515.58] loss=2.18 avg=2.53\n",
            "[220 | 517.81] loss=1.76 avg=2.52\n",
            "[221 | 520.04] loss=2.03 avg=2.52\n",
            "[222 | 522.28] loss=1.69 avg=2.51\n",
            "[223 | 524.51] loss=2.05 avg=2.50\n",
            "[224 | 526.74] loss=1.73 avg=2.49\n",
            "[225 | 528.97] loss=1.77 avg=2.48\n",
            "[226 | 531.19] loss=2.04 avg=2.48\n",
            "[227 | 533.42] loss=1.80 avg=2.47\n",
            "[228 | 535.66] loss=1.61 avg=2.46\n",
            "[229 | 537.89] loss=1.58 avg=2.45\n",
            "[230 | 540.12] loss=2.26 avg=2.45\n",
            "[231 | 542.35] loss=1.78 avg=2.44\n",
            "[232 | 544.58] loss=1.75 avg=2.44\n",
            "[233 | 546.81] loss=1.45 avg=2.42\n",
            "[234 | 549.05] loss=1.76 avg=2.42\n",
            "[235 | 551.28] loss=2.26 avg=2.42\n",
            "[236 | 553.51] loss=1.59 avg=2.41\n",
            "[237 | 555.75] loss=1.35 avg=2.39\n",
            "[238 | 557.98] loss=1.57 avg=2.39\n",
            "[239 | 560.22] loss=1.62 avg=2.38\n",
            "[240 | 562.46] loss=1.73 avg=2.37\n",
            "[241 | 564.69] loss=1.94 avg=2.37\n",
            "[242 | 566.92] loss=1.55 avg=2.36\n",
            "[243 | 569.14] loss=2.00 avg=2.35\n",
            "[244 | 571.38] loss=1.17 avg=2.34\n",
            "[245 | 573.61] loss=1.42 avg=2.33\n",
            "[246 | 575.84] loss=1.76 avg=2.32\n",
            "[247 | 578.08] loss=1.84 avg=2.32\n",
            "[248 | 580.30] loss=1.26 avg=2.31\n",
            "[249 | 582.53] loss=1.66 avg=2.30\n",
            "[250 | 584.77] loss=1.54 avg=2.29\n",
            "[251 | 587.00] loss=1.60 avg=2.28\n",
            "[252 | 589.23] loss=1.64 avg=2.28\n",
            "[253 | 591.46] loss=1.26 avg=2.27\n",
            "[254 | 593.69] loss=1.26 avg=2.25\n",
            "[255 | 595.93] loss=1.25 avg=2.24\n",
            "[256 | 598.16] loss=1.46 avg=2.24\n",
            "[257 | 600.39] loss=1.77 avg=2.23\n",
            "[258 | 602.63] loss=1.29 avg=2.22\n",
            "[259 | 604.85] loss=1.67 avg=2.21\n",
            "[260 | 607.09] loss=1.39 avg=2.21\n",
            "[261 | 609.32] loss=1.52 avg=2.20\n",
            "[262 | 611.55] loss=1.25 avg=2.19\n",
            "[263 | 613.78] loss=1.03 avg=2.18\n",
            "[264 | 616.02] loss=1.61 avg=2.17\n",
            "[265 | 618.25] loss=1.32 avg=2.16\n",
            "[266 | 620.48] loss=1.60 avg=2.15\n",
            "[267 | 622.72] loss=1.23 avg=2.14\n",
            "[268 | 624.95] loss=1.21 avg=2.13\n",
            "[269 | 627.18] loss=1.79 avg=2.13\n",
            "[270 | 629.41] loss=1.62 avg=2.12\n",
            "[271 | 631.64] loss=1.25 avg=2.12\n",
            "[272 | 633.88] loss=1.00 avg=2.10\n",
            "[273 | 636.12] loss=1.35 avg=2.10\n",
            "[274 | 638.35] loss=1.55 avg=2.09\n",
            "[275 | 640.57] loss=1.23 avg=2.08\n",
            "[276 | 642.81] loss=1.31 avg=2.07\n",
            "[277 | 645.04] loss=1.03 avg=2.06\n",
            "[278 | 647.28] loss=1.19 avg=2.05\n",
            "[279 | 649.50] loss=1.04 avg=2.04\n",
            "[280 | 651.73] loss=1.26 avg=2.03\n",
            "[281 | 653.97] loss=1.36 avg=2.03\n",
            "[282 | 656.21] loss=0.96 avg=2.01\n",
            "[283 | 658.45] loss=1.12 avg=2.00\n",
            "[284 | 660.68] loss=1.43 avg=2.00\n",
            "[285 | 662.91] loss=1.21 avg=1.99\n",
            "[286 | 665.14] loss=1.25 avg=1.98\n",
            "[287 | 667.37] loss=1.28 avg=1.97\n",
            "[288 | 669.61] loss=0.98 avg=1.96\n",
            "[289 | 671.85] loss=1.13 avg=1.96\n",
            "[290 | 674.09] loss=0.78 avg=1.94\n",
            "[291 | 676.32] loss=1.28 avg=1.94\n",
            "[292 | 678.56] loss=1.24 avg=1.93\n",
            "[293 | 680.79] loss=0.77 avg=1.92\n",
            "[294 | 683.03] loss=0.79 avg=1.90\n",
            "[295 | 685.26] loss=1.44 avg=1.90\n",
            "[296 | 687.49] loss=1.01 avg=1.89\n",
            "[297 | 689.72] loss=1.25 avg=1.88\n",
            "[298 | 691.95] loss=0.68 avg=1.87\n",
            "[299 | 694.19] loss=1.02 avg=1.86\n",
            "[300 | 696.42] loss=1.07 avg=1.85\n",
            "======== SAMPLE 1 ========\n",
            ", for it has in any case long ago\n",
            "been\n",
            "discredited to philosophasters to arouse their passions in the most incalculable\n",
            "praise and little thought, as from an early age they have been taught\n",
            "that the passions are a bad mulatto, that the best mode of life\n",
            "is therefore thus to become a philosopher, and that this will probably\n",
            "also entail renouncing the passions and renouncing all admirers. That\n",
            "is what has kept philosophers away from the path to happiness which\n",
            "they had chosen, and consequently away from the happiness of the most\n",
            "intellectual and greatest extent of their power:\n",
            "utmost one may look away from these gloomy eyes, and the fact\n",
            "that philosophical thought goes nowhere--NOT AWAY! It never even \"bears the\n",
            "mark.\" On this account it always looks sad.\n",
            "\n",
            "\n",
            "45\n",
            "\n",
            "=Origin of Morality.=--It is unknown why so much distrust exists about\n",
            "valuations, morals, and motives, and why so little\n",
            "harm is done towards those very motives and sentiments, if one recognize\n",
            "so clearly what is apparently so far obscure: altruism,\n",
            "gratitude, unworthiness, naivetÃ©, unsympathy are but the worst\n",
            "perceptions which have been attached to such moralities as \"good\" and \"unworthy,\"\n",
            "and the moral motives so readily admit. To them even this\n",
            "has been left unexplained: why treat others badly, why\n",
            "do they not honour us when we are honouring them? In their\n",
            "suggestion these motives must be the only motives, that is to say, the\n",
            "same as our honour; this seems to them irrestriebriousness, this\n",
            "unwillingness to acknowledge that we are honourable in our own\n",
            "cases, our unegoistic behaviour, their honourable\n",
            "extremism is really our \"faith in God.\" They who would\n",
            "mitigate and disparage, or who harbour dangerous feelings towards\n",
            "our good graces, even in our darkest moments, do not themselves\n",
            "hitherto admit of a CONTRADICTIO IN ADJECTION TO THEM; this\n",
            "morality is first and foremost an allegory, and the COMPLEX morality\n",
            "which has been advanced deprives us of such a deeper and more comprehensive\n",
            "inner morality; hence we still honour and admire the master, we still\n",
            "honour and admire the master who has prepared the way for our\n",
            "treasure. But just as regards gratitude, and the respect of masters,\n",
            "everything else is only an ARGUMENT of greediness and delusion, an\n",
            "ATTACK on the intrinsic reason and on the will to good\n",
            "work; it is precisely precisely precisely the reason, the proper\n",
            "instinct for good looks, of gratitude is only an ADDITIONAL NARROW\n",
            "AGGRESSION in their development; their behaviour is so preposterous\n",
            "that their eyes become bloodshot with hunger and thirst, and they take\n",
            "responsibility for their conduct with such severity that they are led\n",
            "to believe that all human conduct is unethical and therefore immoral\n",
            "(henceforth to \"virtue flouting the law\"), and that from that moment\n",
            "every human, even the gregarious animal, assumes the aspect of a\n",
            "disloyal benefactor, the scholar, the collector, the neighbour's\n",
            "bundle, the pig, and the master, and at the same time feels\n",
            "unrest when the welfare of the community or of the individual\n",
            "is directly involved in his conduct. How can the highly developed man\n",
            "exercise honour when the scholar alone, with his 'work' done,\n",
            "sembles his thoughts and feelings in words sufficient to console himself\n",
            "after his departure from the community or the scholar has left; how\n",
            "sufficient the scholar is, to the individual precisely individual,\n",
            "sympathy and no honour: this, however, is a non-Hegelian\n",
            "error. It is impossible for a man of average family background\n",
            "or education to become a philosopher when he is only about to enter\n",
            "composition, such that his entire outgoing personality is recollected\n",
            "and expressed in his most routine recollections; the whole\n",
            "complex of his conduct is then likewise recollected and\n",
            "refined, and finally his whole future rests upon the recollection of such\n",
            "exceptionally elevated men as he encounters in the street, or in his own\n",
            "company, those 'emirates,' those ostentatious bodies and pictures without\n",
            "a soul beyond its surface, those pert and languorous eyes, which are\n",
            "only at present thinking and estimating, and which could be in the\n",
            "future whole departments of science and art. And this whole process is\n",
            "only stimulating to his thoughts, it is only stimulating to his mood.\n",
            "And thus a thinking man involuntarily delights in the fact\n",
            "that the philosopher always gives the impression that he is not the best\n",
            "performer when he goes about his business; it is not, therefore, his\n",
            "best course of conduct. In\n",
            "\n",
            "[301 | 709.54] loss=1.10 avg=1.85\n",
            "[302 | 711.77] loss=0.92 avg=1.84\n",
            "[303 | 714.00] loss=1.02 avg=1.83\n",
            "[304 | 716.23] loss=0.92 avg=1.82\n",
            "[305 | 718.46] loss=0.90 avg=1.81\n",
            "[306 | 720.69] loss=1.16 avg=1.80\n",
            "[307 | 722.92] loss=0.85 avg=1.79\n",
            "[308 | 725.15] loss=0.81 avg=1.78\n",
            "[309 | 727.38] loss=1.53 avg=1.78\n",
            "[310 | 729.61] loss=1.07 avg=1.77\n",
            "[311 | 731.85] loss=1.09 avg=1.76\n",
            "[312 | 734.07] loss=0.81 avg=1.75\n",
            "[313 | 736.31] loss=0.87 avg=1.74\n",
            "[314 | 738.54] loss=1.18 avg=1.74\n",
            "[315 | 740.77] loss=0.94 avg=1.73\n",
            "[316 | 743.01] loss=0.77 avg=1.72\n",
            "[317 | 745.24] loss=0.94 avg=1.71\n",
            "[318 | 747.47] loss=0.80 avg=1.70\n",
            "[319 | 749.70] loss=0.81 avg=1.69\n",
            "[320 | 751.92] loss=0.89 avg=1.69\n",
            "[321 | 754.15] loss=0.88 avg=1.68\n",
            "[322 | 756.39] loss=0.81 avg=1.67\n",
            "[323 | 758.62] loss=0.65 avg=1.66\n",
            "[324 | 760.84] loss=0.86 avg=1.65\n",
            "[325 | 763.07] loss=0.76 avg=1.64\n",
            "[326 | 765.30] loss=0.79 avg=1.63\n",
            "[327 | 767.53] loss=1.13 avg=1.63\n",
            "[328 | 769.76] loss=0.76 avg=1.62\n",
            "[329 | 771.99] loss=0.69 avg=1.61\n",
            "[330 | 774.22] loss=0.78 avg=1.60\n",
            "[331 | 776.45] loss=0.97 avg=1.59\n",
            "[332 | 778.69] loss=0.59 avg=1.58\n",
            "[333 | 780.92] loss=0.88 avg=1.57\n",
            "[334 | 783.15] loss=0.61 avg=1.56\n",
            "[335 | 785.38] loss=0.91 avg=1.56\n",
            "[336 | 787.62] loss=0.66 avg=1.55\n",
            "[337 | 789.85] loss=0.67 avg=1.54\n",
            "[338 | 792.08] loss=0.83 avg=1.53\n",
            "[339 | 794.31] loss=0.64 avg=1.52\n",
            "[340 | 796.54] loss=0.58 avg=1.51\n",
            "[341 | 798.77] loss=0.83 avg=1.51\n",
            "[342 | 801.01] loss=0.59 avg=1.50\n",
            "[343 | 803.25] loss=0.76 avg=1.49\n",
            "[344 | 805.48] loss=0.91 avg=1.48\n",
            "[345 | 807.70] loss=0.74 avg=1.47\n",
            "[346 | 809.93] loss=0.60 avg=1.47\n",
            "[347 | 812.16] loss=0.57 avg=1.46\n",
            "[348 | 814.40] loss=0.78 avg=1.45\n",
            "[349 | 816.64] loss=0.75 avg=1.44\n",
            "[350 | 818.87] loss=0.68 avg=1.43\n",
            "[351 | 821.10] loss=0.63 avg=1.43\n",
            "[352 | 823.33] loss=0.61 avg=1.42\n",
            "[353 | 825.56] loss=0.71 avg=1.41\n",
            "[354 | 827.79] loss=0.65 avg=1.40\n",
            "[355 | 830.02] loss=0.71 avg=1.40\n",
            "[356 | 832.25] loss=1.11 avg=1.39\n",
            "[357 | 834.48] loss=0.60 avg=1.38\n",
            "[358 | 836.71] loss=0.42 avg=1.37\n",
            "[359 | 838.94] loss=0.42 avg=1.36\n",
            "[360 | 841.17] loss=0.59 avg=1.36\n",
            "[361 | 843.40] loss=0.60 avg=1.35\n",
            "[362 | 845.63] loss=0.71 avg=1.34\n",
            "[363 | 847.87] loss=0.67 avg=1.34\n",
            "[364 | 850.10] loss=0.50 avg=1.33\n",
            "[365 | 852.33] loss=0.46 avg=1.32\n",
            "[366 | 854.56] loss=0.53 avg=1.31\n",
            "[367 | 856.79] loss=0.55 avg=1.30\n",
            "[368 | 859.03] loss=0.89 avg=1.30\n",
            "[369 | 861.27] loss=0.47 avg=1.29\n",
            "[370 | 863.51] loss=0.51 avg=1.28\n",
            "[371 | 865.73] loss=0.43 avg=1.27\n",
            "[372 | 867.97] loss=0.33 avg=1.26\n",
            "[373 | 870.19] loss=0.59 avg=1.26\n",
            "[374 | 872.43] loss=0.41 avg=1.25\n",
            "[375 | 874.67] loss=0.36 avg=1.24\n",
            "[376 | 876.91] loss=0.62 avg=1.23\n",
            "[377 | 879.14] loss=0.47 avg=1.22\n",
            "[378 | 881.38] loss=0.59 avg=1.22\n",
            "[379 | 883.61] loss=0.54 avg=1.21\n",
            "[380 | 885.84] loss=0.72 avg=1.21\n",
            "[381 | 888.07] loss=0.38 avg=1.20\n",
            "[382 | 890.31] loss=0.46 avg=1.19\n",
            "[383 | 892.53] loss=0.49 avg=1.18\n",
            "[384 | 894.76] loss=0.25 avg=1.17\n",
            "[385 | 896.99] loss=0.37 avg=1.17\n",
            "[386 | 899.23] loss=0.88 avg=1.16\n",
            "[387 | 901.46] loss=0.32 avg=1.15\n",
            "[388 | 903.69] loss=0.66 avg=1.15\n",
            "[389 | 905.93] loss=0.27 avg=1.14\n",
            "[390 | 908.16] loss=1.06 avg=1.14\n",
            "[391 | 910.40] loss=0.36 avg=1.13\n",
            "[392 | 912.64] loss=0.39 avg=1.12\n",
            "[393 | 914.87] loss=0.45 avg=1.12\n",
            "[394 | 917.10] loss=0.91 avg=1.11\n",
            "[395 | 919.33] loss=0.53 avg=1.11\n",
            "[396 | 921.57] loss=0.40 avg=1.10\n",
            "[397 | 923.80] loss=0.48 avg=1.09\n",
            "[398 | 926.03] loss=0.25 avg=1.09\n",
            "[399 | 928.27] loss=0.54 avg=1.08\n",
            "[400 | 930.50] loss=0.27 avg=1.07\n",
            "======== SAMPLE 1 ========\n",
            " of religious fundamental law, and to call for\n",
            "help in relation to that law, and for the rearing of charitable\n",
            "superstitions therein (the \"fundamental problem,\" as the philosophers call it)\n",
            "will, on the other hand, be impossible unless a dogma as to\n",
            "fundamental can be established, and if an exception (the inspired\n",
            "belief of the saints in those rarer hours and rarer\n",
            "days of the week) can nevertheless be made of it, from the\n",
            "dominion of pride and obligation in itself, which is impossible on\n",
            "account of the young age, when reflection is the mother of\n",
            "society. It is the absolute evil of individuals to wander about like\n",
            "therskin and petty, as though new belief be the deciding factor,\n",
            "and there also of course also to be irresponsible and selfish:\n",
            "one can feel one's responsibility for one's acts, as much as one\n",
            "considers good and evil, in one's personality; one can be hardy,\n",
            "loyal, and WILL TO RECOGNISE; one can IGNORE one's virtues (know\n",
            "so said from time to time); one can stand firmly and treasureously\n",
            "on one's own assets, in one's possession-animalism: these\n",
            "are only a few of the many virtues which the christian man has\n",
            "to offer himself.\" The great joy of the christian man, to receive from\n",
            "us, with one another, unqualified assurance that he will\n",
            "stand firm, and really do something, in the end, is due to the\n",
            "extraordinary self-confidence of the semi-divine Sacrificialer in his christian\n",
            "condition: he cannot allow himself to be falsely sworn to\n",
            "such things. In truth, the more deeply the natures suffer from the\n",
            "influences of religious emotion, the more supercilious become against\n",
            "these feelings the more readily can the natures be overcome by the will of\n",
            "god. In order to overcome unbelief it is sufficient to spiritualize\n",
            "themselves into high spirituality and religious mysticism; also to make them\n",
            "very substantial elements in the very great systems of religion which\n",
            "are to be found in every Continent, in every nation-spirit, in everything\n",
            "which is to mediate between these peoples and each other--including\n",
            "in the highest and most delicate degree into the nations only into\n",
            "the nations only the will of god is established, the very idea of a God\n",
            "(that is to say an \"impulse\" for the peoples as a whole) finds expression\n",
            "in the idea of a homelandship, in the notion of a common destiny, in\n",
            "what is called \"ideas,\" even in art and literature into the\n",
            "treasure of this impulse. Let it at once be noted that the christian man,\n",
            "through sheer religious emotion and semi-divine Will to Power, has\n",
            "succeeded in so far as to make all his ideas into streams, thus becoming\n",
            "a type of eternal grammar; he has succeeded in putting on the gait\n",
            "towards reality, towards which all other kinds of fomenting and\n",
            "rearing distress are subjected. The religious man, indeed, if\n",
            "disinterested in truth, looks on everything that exists, all the things\n",
            "that make the world go round, round and in every way connected with these\n",
            "things as if everything in them were powers, as the conceited man\n",
            "is little disposed to comprehend; he thinks the same about everything\n",
            "that pertains to the most extraordinary, the most inexplicable, the\n",
            "most wonderful and the most delightful things. Yet how much does\n",
            "such a man understand that he himself is the cause of a whole series of\n",
            "effects, and that what seems miraculous to him is at the same\n",
            "time something which seems delightful to men merely by\n",
            "consideration--for though there be sunshine and forest-cleaning and all\n",
            "the marvels of nature, there would be also a sort of sleaze, a sort of\n",
            "snarescope to the senses, some sort of repulsive and terrifying\n",
            "eriegrass on the part of man. Here and there he understands deeply, very\n",
            "deeply, but never enough to put his finger on what is really\n",
            "really happening--his sympathy with it.\n",
            "\n",
            "\n",
            "97\n",
            "\n",
            "=The Dream.=--As the hours draw near, the keenest acuteness of our\n",
            "emotions stirs them. We are awake, anxious, furious,,, with\n",
            "inherent dread, so the dream (wakingusa luddei) has led us to believe. How\n",
            "many kinds of waking thought are entangled in the mythology of\n",
            "old men: they cling to the notion of a \"rising\" and of an \"ending\"\n",
            "(timbriandace). Whichever of their relations there is, what of\n",
            "that which constitutes the term \"rising,\" does not constitute the\n",
            "enduring feeling of joy, of enjoyment, of perhaps alleviating or\n",
            "giving a\n",
            "\n",
            "[401 | 943.62] loss=0.46 avg=1.07\n",
            "[402 | 945.85] loss=0.42 avg=1.06\n",
            "[403 | 948.09] loss=0.50 avg=1.05\n",
            "[404 | 950.32] loss=0.71 avg=1.05\n",
            "[405 | 952.55] loss=0.41 avg=1.04\n",
            "[406 | 954.78] loss=0.47 avg=1.04\n",
            "[407 | 957.01] loss=0.39 avg=1.03\n",
            "[408 | 959.25] loss=0.37 avg=1.02\n",
            "[409 | 961.49] loss=0.44 avg=1.02\n",
            "[410 | 963.72] loss=0.27 avg=1.01\n",
            "[411 | 965.96] loss=0.56 avg=1.01\n",
            "[412 | 968.18] loss=0.41 avg=1.00\n",
            "[413 | 970.42] loss=0.29 avg=0.99\n",
            "[414 | 972.66] loss=0.45 avg=0.99\n",
            "[415 | 974.89] loss=0.33 avg=0.98\n",
            "[416 | 977.11] loss=0.47 avg=0.98\n",
            "[417 | 979.35] loss=0.43 avg=0.97\n",
            "[418 | 981.58] loss=0.29 avg=0.96\n",
            "[419 | 983.82] loss=0.24 avg=0.96\n",
            "[420 | 986.04] loss=0.28 avg=0.95\n",
            "[421 | 988.27] loss=0.31 avg=0.94\n",
            "[422 | 990.50] loss=0.29 avg=0.94\n",
            "[423 | 992.73] loss=0.23 avg=0.93\n",
            "[424 | 994.96] loss=0.28 avg=0.92\n",
            "[425 | 997.19] loss=0.33 avg=0.92\n",
            "[426 | 999.42] loss=0.37 avg=0.91\n",
            "[427 | 1001.65] loss=0.35 avg=0.91\n",
            "[428 | 1003.87] loss=0.26 avg=0.90\n",
            "[429 | 1006.10] loss=0.42 avg=0.89\n",
            "[430 | 1008.34] loss=0.32 avg=0.89\n",
            "[431 | 1010.57] loss=0.23 avg=0.88\n",
            "[432 | 1012.80] loss=0.30 avg=0.88\n",
            "[433 | 1015.03] loss=0.37 avg=0.87\n",
            "[434 | 1017.26] loss=0.62 avg=0.87\n",
            "[435 | 1019.49] loss=0.23 avg=0.86\n",
            "[436 | 1021.72] loss=0.32 avg=0.86\n",
            "[437 | 1023.95] loss=0.21 avg=0.85\n",
            "[438 | 1026.17] loss=0.31 avg=0.84\n",
            "[439 | 1028.40] loss=0.60 avg=0.84\n",
            "[440 | 1030.63] loss=0.36 avg=0.84\n",
            "[441 | 1032.87] loss=0.49 avg=0.83\n",
            "[442 | 1035.09] loss=0.81 avg=0.83\n",
            "[443 | 1037.33] loss=0.25 avg=0.83\n",
            "[444 | 1039.56] loss=0.24 avg=0.82\n",
            "[445 | 1041.79] loss=0.33 avg=0.82\n",
            "[446 | 1044.02] loss=0.40 avg=0.81\n",
            "[447 | 1046.25] loss=0.30 avg=0.81\n",
            "[448 | 1048.48] loss=0.29 avg=0.80\n",
            "[449 | 1050.71] loss=0.31 avg=0.80\n",
            "[450 | 1052.94] loss=0.32 avg=0.79\n",
            "[451 | 1055.17] loss=0.18 avg=0.79\n",
            "[452 | 1057.41] loss=0.51 avg=0.78\n",
            "[453 | 1059.65] loss=0.29 avg=0.78\n",
            "[454 | 1061.88] loss=0.33 avg=0.77\n",
            "[455 | 1064.11] loss=0.25 avg=0.77\n",
            "[456 | 1066.34] loss=0.38 avg=0.76\n",
            "[457 | 1068.58] loss=0.27 avg=0.76\n",
            "[458 | 1070.81] loss=0.25 avg=0.75\n",
            "[459 | 1073.04] loss=0.19 avg=0.75\n",
            "[460 | 1075.27] loss=0.36 avg=0.74\n",
            "[461 | 1077.50] loss=0.22 avg=0.74\n",
            "[462 | 1079.74] loss=0.17 avg=0.73\n",
            "[463 | 1081.97] loss=0.35 avg=0.73\n",
            "[464 | 1084.21] loss=0.29 avg=0.72\n",
            "[465 | 1086.43] loss=0.20 avg=0.72\n",
            "[466 | 1088.67] loss=0.27 avg=0.71\n",
            "[467 | 1090.90] loss=0.34 avg=0.71\n",
            "[468 | 1093.14] loss=0.28 avg=0.71\n",
            "[469 | 1095.37] loss=0.17 avg=0.70\n",
            "[470 | 1097.60] loss=0.35 avg=0.70\n",
            "[471 | 1099.83] loss=0.20 avg=0.69\n",
            "[472 | 1102.07] loss=0.27 avg=0.69\n",
            "[473 | 1104.31] loss=0.30 avg=0.68\n",
            "[474 | 1106.54] loss=0.19 avg=0.68\n",
            "[475 | 1108.76] loss=0.23 avg=0.67\n",
            "[476 | 1111.00] loss=0.16 avg=0.67\n",
            "[477 | 1113.23] loss=0.21 avg=0.67\n",
            "[478 | 1115.46] loss=0.18 avg=0.66\n",
            "[479 | 1117.70] loss=0.34 avg=0.66\n",
            "[480 | 1119.94] loss=0.18 avg=0.65\n",
            "[481 | 1122.17] loss=0.22 avg=0.65\n",
            "[482 | 1124.40] loss=0.21 avg=0.64\n",
            "[483 | 1126.63] loss=0.23 avg=0.64\n",
            "[484 | 1128.87] loss=0.22 avg=0.63\n",
            "[485 | 1131.10] loss=0.23 avg=0.63\n",
            "[486 | 1133.34] loss=0.14 avg=0.63\n",
            "[487 | 1135.57] loss=0.30 avg=0.62\n",
            "[488 | 1137.80] loss=0.16 avg=0.62\n",
            "[489 | 1140.05] loss=0.21 avg=0.61\n",
            "[490 | 1142.29] loss=0.23 avg=0.61\n",
            "[491 | 1144.52] loss=0.22 avg=0.61\n",
            "[492 | 1146.75] loss=0.23 avg=0.60\n",
            "[493 | 1148.98] loss=0.19 avg=0.60\n",
            "[494 | 1151.22] loss=0.20 avg=0.59\n",
            "[495 | 1153.45] loss=0.30 avg=0.59\n",
            "[496 | 1155.69] loss=0.14 avg=0.59\n",
            "[497 | 1157.92] loss=0.20 avg=0.58\n",
            "[498 | 1160.16] loss=0.19 avg=0.58\n",
            "[499 | 1162.39] loss=0.16 avg=0.57\n",
            "[500 | 1164.63] loss=0.23 avg=0.57\n",
            "Saving checkpoint/run1/model-500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text generation"
      ],
      "metadata": {
        "id": "bUagiJzBTeoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"Something is hidden\""
      ],
      "metadata": {
        "id": "qzTK7bdIPeOY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.generate(sess, prefix=prefix, length=150)"
      ],
      "metadata": {
        "id": "ZCaaNXR7kI9I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ca2e646-9830-45e3-93c1-a33f19f7a5cc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Something is hidden in the shame of the\n",
            "highest god. It is not the degree of stupidity which\n",
            "desires of mortals; the highest degree of stupidity is the\n",
            "highest egoism. Above all a being who is sinless and has\n",
            "nothing to do against him, who has the Will to Truth, who is the EXISTENCE of\n",
            "the Truth, knows only the PATH of a MAN and knows only the descent\n",
            "of a man; above all a being who is sinless, who has nothing to do\n",
            "against him, who has the Virtue of Non-Sinus Sanctus, knows only the\n",
            "PATHS of a man. The shame of the highest stupidity is that it is\n",
            "insight-deprivation that enchants.si\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Saving model to Google Drive (optional)"
      ],
      "metadata": {
        "id": "zlM6aQYZSccl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYXmOFl5Bjhv",
        "outputId": "564ebb74-2ba5-403f-dc4b-c36ce1478d52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run1')"
      ],
      "metadata": {
        "id": "3RUjr4_ZluKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find more texts e.g. on:\n",
        "https://www.gutenberg.org/cache/epub/1597/pg1597.txt\n",
        "</br></br>\n",
        "You can download them to Colab using code similar to the ones below."
      ],
      "metadata": {
        "id": "OUhaGg_uS6o5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://www.gutenberg.org/cache/epub/1597/pg1597.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7K9X3K8TEwj",
        "outputId": "d0760c42-a0e4-4dcf-b7cc-ca98aaffa2b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-21 14:49:16--  https://www.gutenberg.org/cache/epub/1597/pg1597.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 329071 (321K) [text/plain]\n",
            "Saving to: â€˜pg1597.txtâ€™\n",
            "\n",
            "pg1597.txt          100%[===================>] 321.36K   800KB/s    in 0.4s    \n",
            "\n",
            "2023-03-21 14:49:22 (800 KB/s) - â€˜pg1597.txtâ€™ saved [329071/329071]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://www.gutenberg.org/files/98/98-0.txt"
      ],
      "metadata": {
        "id": "HYL0wij2m4Gf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42bf360b-ce90-4a36-d434-44820124b877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-22 13:25:10--  https://www.gutenberg.org/files/98/98-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 807231 (788K) [text/plain]\n",
            "Saving to: â€˜98-0.txtâ€™\n",
            "\n",
            "98-0.txt            100%[===================>] 788.31K   718KB/s    in 1.1s    \n",
            "\n",
            "2023-02-22 13:25:12 (718 KB/s) - â€˜98-0.txtâ€™ saved [807231/807231]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/matt-dray/tng-stardate/tree/master/data/scripts"
      ],
      "metadata": {
        "id": "VClsbkgRxYvR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}